{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2f06c6-ee79-44e2-a982-ecf1bc07d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7713594b-ad3b-4f1e-86ce-6ec4db44f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file for \n",
    "\n",
    "df = pd.read_csv('online_retail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50c24d1-75eb-4333-8e12-cf6c8782280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the dataset into another variable, to use and make changes, by not affecting the original dataset\n",
    "\n",
    "or_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0530bd-0177-40fb-a71f-c962523d6bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>536365</td>\n",
       "      <td>22752</td>\n",
       "      <td>SET 7 BABUSHKA NESTING BOXES</td>\n",
       "      <td>2</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>7.65</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>536365</td>\n",
       "      <td>21730</td>\n",
       "      <td>GLASS STAR FROSTED T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>4.25</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>536366</td>\n",
       "      <td>22633</td>\n",
       "      <td>HAND WARMER UNION JACK</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:28</td>\n",
       "      <td>1.85</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>536366</td>\n",
       "      <td>22632</td>\n",
       "      <td>HAND WARMER RED POLKA DOT</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:28</td>\n",
       "      <td>1.85</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>536367</td>\n",
       "      <td>84879</td>\n",
       "      <td>ASSORTED COLOUR BIRD ORNAMENT</td>\n",
       "      <td>32</td>\n",
       "      <td>12/1/2010 8:34</td>\n",
       "      <td>1.69</td>\n",
       "      <td>13047.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "5    536365     22752         SET 7 BABUSHKA NESTING BOXES         2   \n",
       "6    536365     21730    GLASS STAR FROSTED T-LIGHT HOLDER         6   \n",
       "7    536366     22633               HAND WARMER UNION JACK         6   \n",
       "8    536366     22632            HAND WARMER RED POLKA DOT         6   \n",
       "9    536367     84879        ASSORTED COLOUR BIRD ORNAMENT        32   \n",
       "\n",
       "      InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
       "1  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "2  12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
       "3  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "4  12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "5  12/1/2010 8:26       7.65     17850.0  United Kingdom  \n",
       "6  12/1/2010 8:26       4.25     17850.0  United Kingdom  \n",
       "7  12/1/2010 8:28       1.85     17850.0  United Kingdom  \n",
       "8  12/1/2010 8:28       1.85     17850.0  United Kingdom  \n",
       "9  12/1/2010 8:34       1.69     13047.0  United Kingdom  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top 10 Rows\n",
    "\n",
    "or_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22baaa05-4cf6-42dd-9007-b7da6274d9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(541909, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the dataframe\n",
    "\n",
    "or_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f68031-982f-42c9-8b2e-3baa02718f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=541909, step=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The index of the dataframe\n",
    "\n",
    "or_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971249dd-6f95-4f02-b561-98e0b2f51d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID', 'Country']\n"
     ]
    }
   ],
   "source": [
    "# The list of column names present in the dataframe\n",
    "\n",
    "print(list(or_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6d882-dc93-4a6e-ac4d-01ef9e0cd41b",
   "metadata": {},
   "source": [
    "## Attributes Description\n",
    "\n",
    "There are 8 attributes. They are:\n",
    "\n",
    "1) InvoiceNo - Unique identifier for each transaction.\n",
    "2) StockCode - Unique product code for an item.\n",
    "3) Description - Description of the product.\n",
    "4) Quantity - Number of units purchased.\n",
    "5) InvoiceDate - Date and time of the transaction.\n",
    "6) UnitPrice - Cost of a single unit of the product.\n",
    "7) CustomerID - Unique identifier for each customer.\n",
    "8) Country - Country where the customer lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d920dc0d-5411-40fb-8fe4-6cbe17f30e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Information about the DataFrame\n",
    "\n",
    "or_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b0cff-418f-4031-9e1c-23fb0f807c8f",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9ee0b0-2476-46c7-b85b-ec5c3be16a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo           0\n",
       "StockCode           0\n",
       "Description      1454\n",
       "Quantity            0\n",
       "InvoiceDate         0\n",
       "UnitPrice           0\n",
       "CustomerID     135080\n",
       "Country             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null/missing values\n",
    "\n",
    "or_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956221c7-9018-4edd-adc5-eaaba6941913",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "There are two attributes : Description and CustomerID that contains the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073910cb-0d8e-462c-b9e9-93431d737cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the column Description \n",
    "\n",
    "or_data.drop(columns=['Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c84e3-7d15-41bb-9ba9-e2017b0a70d4",
   "metadata": {},
   "source": [
    "### Description \n",
    "\n",
    "There attribute description has been dropped because it isn't necessary for analysing customer purchasing behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504ac238-5675-4347-ac86-783bc80c2b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset after dropping Description column :  (541909, 7) \n",
      "\n",
      "The size of the dataset before dropping Description column :  (541909, 8) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode  Quantity     InvoiceDate  UnitPrice  CustomerID  \\\n",
       "0    536365    85123A         6  12/1/2010 8:26       2.55     17850.0   \n",
       "1    536365     71053         6  12/1/2010 8:26       3.39     17850.0   \n",
       "2    536365    84406B         8  12/1/2010 8:26       2.75     17850.0   \n",
       "3    536365    84029G         6  12/1/2010 8:26       3.39     17850.0   \n",
       "4    536365    84029E         6  12/1/2010 8:26       3.39     17850.0   \n",
       "\n",
       "          Country  \n",
       "0  United Kingdom  \n",
       "1  United Kingdom  \n",
       "2  United Kingdom  \n",
       "3  United Kingdom  \n",
       "4  United Kingdom  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape of the dataframe after dropping Description column\n",
    "\n",
    "print(\"The size of the dataset after dropping Description column : \",or_data.shape,\"\\n\")\n",
    "\n",
    "# The original shape of the dataframe\n",
    "\n",
    "print(\"The size of the dataset before dropping Description column : \",df.shape,\"\\n\")\n",
    "\n",
    "# Display Top 5 Rows\n",
    "\n",
    "or_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ebf13-2fc1-414e-8b5a-1dc75e85d440",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "When observing the shape of the dataframe, we can see that there is reduction in the column, i.e., our data set had 8 columns and after removing the description column we can see there are 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caee1be4-170a-4e48-b5fe-4c0830e0e1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo           0\n",
       "StockCode           0\n",
       "Quantity            0\n",
       "InvoiceDate         0\n",
       "UnitPrice           0\n",
       "CustomerID     135080\n",
       "Country             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null/missing values \n",
    "\n",
    "or_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa667327-0e71-4e8e-b355-f332e732c255",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "There is only one attribute CustomerID that contains the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035db85-f2c4-4a2c-a51b-d09d18abbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data to ensure consistency\n",
    "\n",
    "or_data.sort_values(by=['InvoiceNo', 'StockCode', 'InvoiceDate', 'Quantity', 'UnitPrice', 'Country'], inplace=True)\n",
    "\n",
    "# Remove the rows in CustomerID that contain null values\n",
    "\n",
    "or_data.dropna(subset=['CustomerID'],axis=0,inplace=True)\n",
    "\n",
    "# Check null/missing values \n",
    "\n",
    "or_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23609b2c-6449-45e7-b079-b747cdd56c07",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "After sorting data there was an observation there was no matching rows when gouped by InvoiceNo,StockCode,InvoiceDate,Quantity,UnitPrice,Country, these columns didn't have same CustomerID i.e., each CustomerID had only a single row which contained null values, so the best approach to filter the CustomerID column was to drop all the missing/null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53861aed-8787-4d4f-90e4-49a10f10d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of the dataframe\n",
    "\n",
    "or_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2af25-b257-4ec1-b289-ccc2736753a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the duplicate values using group and aggregation\n",
    "\n",
    "or_data = or_data.groupby(['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID', 'Country']).agg(\n",
    "          Quantity=('Quantity', 'sum'), UnitPrice=('UnitPrice','mean'), Count=('Quantity','size')).reset_index()\n",
    "\n",
    "# Rows present after removing duplicates\n",
    "\n",
    "print(\"Total number of rows after removing duplicates:\", len(or_data), \"\\n\")\n",
    "\n",
    "# Display first 10 rows \n",
    "\n",
    "or_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8ad54-cbf3-46b1-860a-c55305087a24",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "In this code, checking the duplicates for the selected columns and using aggregation function to sum up the quantity based on the columns specified, i.e., the rows  that are similar based on InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country' if the unit price is same, it's grouped, if unit price varies based on the columns that are grouped then the mean of the unit price is taken because a customer who made one large discounted purchase should not be treated differently from a customer who made the same purchase split into multiple rows as our goal is goal is to segment customers based on purchasing behavior, not pricing fluctuations. Also column count is added, shows the occurance of the rows, since there are count values = 1, we can say that it has removed all the duplicate values that was present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d5e14-051f-4c0d-8f36-265e0f04f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Count column, and check the shape of the dataframe after removing the duplicates and display first five rows\n",
    "\n",
    "or_data.drop(columns=['Count'],inplace=True)\n",
    "print(\"The size of the dataset after removing duplicates : \",or_data.shape,\"\\n\")\n",
    "or_data.head()\n",
    "\n",
    "# Cross-Check for duplicates\n",
    "\n",
    "duplicates = or_data.duplicated()\n",
    "number_duplicates = duplicates.sum()\n",
    "print(\"Total duplicates present: \",number_duplicates,\"\\n\")\n",
    "\n",
    "# Display first 10 rows \n",
    "or_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fc6b4-f398-42a6-9025-183d7fc448cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Negative integer values present in Quantity and then display it\n",
    "\n",
    "neg_quantity = or_data[or_data[\"Quantity\"]<0]\n",
    "print(\"Count of rows in quantity having negative values : \",len(neg_quantity),\"\\n\")\n",
    "neg_quantity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a55ec6-0df1-4e77-a849-22ec966cd967",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "There are total of 8806 rows that contains negative integer value for the quantity, also we can observe in the invoice column the id starts with C which implies Returns/Cancellations done by the customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6e081-2520-401d-b84f-dfa41dd4b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the dataset into another variable, to use and make changes, by not affecting the cleaned dataset\n",
    "\n",
    "or_data = or_data[or_data['Quantity']>=0]\n",
    "\n",
    "#Display the dataset\n",
    "\n",
    "print(\"Count of rows in quantity having positive values : \",len(or_data),\"\\n\")\n",
    "\n",
    "or_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfec5ab-5b60-4971-9369-de19c89b9c77",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "In the or_data, it contains only the dataset with rows that contains only the positive values of the quantity, we tend to remove the negative quantity and invoices that are either cancelled because including it would cause issues regarding total money spent on the product and how often they made actual purchases as we are trying to find customers based on the purchasing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec10ccc-9969-49bb-b72f-a87abb7cb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Checking the outliers by selecting the features from customer level insights\n",
    "\n",
    "customer_metrics1 = ['Quantity', 'UnitPrice']\n",
    "\n",
    "\n",
    "# Checking if the selected features exist in the dataset using list comprehension\n",
    "\n",
    "customer_metrics1 = [col for col in customer_metrics1 if col in or_data.columns]\n",
    "\n",
    "# Plotting boxplot\n",
    "\n",
    "for col in customer_metrics1 : \n",
    "    plt.figure(figsize=(8,2))\n",
    "    sns.boxplot(x=or_data[col])\n",
    "    plt.title('Outliers')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219901c-1651-4287-b9a0-9683401746df",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "Both Quantity and UnitPrice distributions show numerous extreme outliers far beyond the typical range, suggesting a small number of transactions have exceptionally high quantities or unit prices that warrant investigation or treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9bf3f-ff31-4e3f-a543-f21ed39cbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Outliers using IQR\n",
    "\n",
    "# Compute IQR & selecting the features Quantity & UnitPrice\n",
    "\n",
    "features = ['Quantity', 'UnitPrice']\n",
    "\n",
    "Q1 = or_data[features].quantile(0.25)\n",
    "Q3 = or_data[features].quantile(0.75)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Capping outliers at the 90 percentile\n",
    "\n",
    "cap = or_data[features].quantile(0.90)\n",
    "\n",
    "for col in features:\n",
    "    or_data[col] = np.where(or_data[col] > upper_bound[col], cap[col], or_data[col])\n",
    "\n",
    "# Plotting boxplot\n",
    "\n",
    "for col in features : \n",
    "    plt.figure(figsize=(8,2))\n",
    "    sns.boxplot(x=or_data[col])\n",
    "    plt.title('Outliers')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19064af-2df7-4146-aa3a-720b868814cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d87fc-6d65-43b9-bdc8-0c9c447f6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761dcc5-dc1b-441d-96b5-bad8d5dc1fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the dataset into another variable, to use and make changes, by not affecting the cleaned dataset\n",
    "\n",
    "purchased_data = or_data.copy()\n",
    "\n",
    "# The shape of the dataframe after removing the negative values in the Quantity column\n",
    "\n",
    "print(\"The size of the dataset :\",purchased_data.shape,\"\\n\")\n",
    "\n",
    "#Display the dataset \n",
    "\n",
    "purchased_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7529063-1929-4555-9cca-52ed3badc08e",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e9e5d-001e-4d00-b190-112cc5b59685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find Recency,Frequency,Monetary, ProductVariety, Tenure, AOV\n",
    "\n",
    "# Convert Customer Id ti int\n",
    "\n",
    "purchased_data['CustomerID'] = purchased_data['CustomerID'].astype(int)\n",
    "\n",
    "# Ensure the Invoice Date is in datetime format\n",
    "\n",
    "purchased_data['InvoiceDate']=pd.to_datetime(purchased_data['InvoiceDate'])\n",
    "\n",
    "# Calculating the latest purchase for using it in recency\n",
    "\n",
    "latest_purchase = purchased_data['InvoiceDate'].max()\n",
    "\n",
    "# Calculate Total Cost using quantity and unit price\n",
    "\n",
    "purchased_data['TotalCost'] = purchased_data['Quantity'] * purchased_data['UnitPrice']\n",
    "\n",
    "# Calculate Recency, Frequency, Monetary, AOV, ProductVariety, Tenure metrics by grouping data by Customer ID\n",
    "\n",
    "purchased_data = purchased_data.groupby('CustomerID').agg (\n",
    "                Recency = ('InvoiceDate', lambda x: (latest_purchase - x.max()).days),\n",
    "                Frequency = ('InvoiceNo','nunique'),\n",
    "                Monetary = ('TotalCost','sum'),\n",
    "                ProductVariety = ('StockCode','nunique'),\n",
    "                Tenure=('InvoiceDate', lambda x: (x.max() - x.min()).days),\n",
    "                ).reset_index()\n",
    "\n",
    "purchased_data['AOV'] = purchased_data['Monetary'] / purchased_data['Frequency']\n",
    "\n",
    "purchased_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067aa98-bd7a-481a-9127-141c8a19e1b2",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "We have done the grouping of customerid, to convert transaction level data into customer level insights, as this is essential for segmentation.\n",
    "\n",
    "1) **Recency**\t: The recency tells us how many days ago the customer made purchase, it is used for finding the recent active customer and we calculate    the days between the absolute latest date and the customer's latest date using InvoiceDate\n",
    "2) **Frequency** :\tThe frequency tells us the sepearte transactions the customer has made, it is used for identifying the customers who purchase often    and to calculate frequency we count the number of unique values of the InvoiceNo for the customers.\n",
    "3) **Monetary** : The Monetary gives us the total amount of money the customer has spent across all purchases, it is used for identifying the high-        spending customers and to calculate Monetary we take the total cost we multiply using quantity and unit price.\n",
    "4) **ProductVariety** : The AOV tells us how many different unique products the customer has bought, it is used to see if a customer buys a wide range     of items or focuses on a few and to calculate ProductVariety we count the number of unique values of the StockCode for the customers.\n",
    "5) **Tenure** : The frequency tells us the number of days between the customer's very first and very last recorded purchase, it is used for measuring      how long the customer has been actively purchasing within the dataset's timeframe and to calculate Tenure we consider the days between the customers    minimum InvoiceDate and maximum InvoiceDate.         \n",
    "6) **AOV (Average Order Value)** : The AOV gives us the average amount spent by the customer each time they make a purchase, it is used to understand       the typical transaction size for a customer and to calculate AOV we divide the customer's total Monetary value by their Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf735f-1bb7-4c28-828e-0f6451ab0fd0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda88aa2-8c27-4ba8-b1e6-fa0f1f46f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "purchased_data[['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'Tenure', 'AOV']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a70a2c-46c7-404d-9ab6-f0a962d86a20",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "In the descriptive statitics for each of the customer level features calculated the mean is much higher than the 50% value which is the median and for the 75% value the customers are spending less. So this shows that our data is right skewed. Also we can observe for the 75% value is far away from the maximum value, of the data such as monetary and average order value that tells us, this dataset contains outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cf140-91ba-41dd-a05a-729ef81050ca",
   "metadata": {},
   "source": [
    "# Handing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe47db64-34ee-4603-a2d4-5ba92d6f22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Checking the outliers by selecting the features from customer level insights\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'Tenure', 'AOV']\n",
    "\n",
    "# Checking if the selected features exist in the dataset using list comprehension\n",
    "\n",
    "customer_metrics = [col for col in customer_metrics if col in purchased_data.columns]\n",
    "\n",
    "# Plotting boxplot\n",
    "\n",
    "for col in customer_metrics : \n",
    "    plt.figure(figsize=(8,2))\n",
    "    sns.boxplot(x=purchased_data[col])\n",
    "    plt.title('Outliers')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d21f8-ee0c-4244-829c-d7dd5e649778",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "We can observe the box plots that indicates most features in the dataset, especially Frequency, Monetary, ProductVariety and AOV has high number of outliers, this suggests that some customers have high purchase activity compared to the majority. As you can see Tenure has no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56f0a7-a1cf-40c1-8d70-360b731e5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Outliers using IQR\n",
    "\n",
    "# Compute IQR & selecting the features from customer level insights\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'AOV']\n",
    "\n",
    "Q1 = purchased_data[customer_metrics].quantile(0.25)\n",
    "Q3 = purchased_data[customer_metrics].quantile(0.75)\n",
    "\n",
    "IQR = Q3-Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Capping outliers at the 90 percentile\n",
    "\n",
    "cap = purchased_data[customer_metrics].quantile(0.90)\n",
    "for col in customer_metrics:\n",
    "    purchased_data[col] = np.where(purchased_data[col] > upper_bound[col], cap[col], purchased_data[col])\n",
    "\n",
    "# Plotting boxplot\n",
    "\n",
    "for col in customer_metrics : \n",
    "    plt.figure(figsize=(8,2))\n",
    "    sns.boxplot(x=purchased_data[col])\n",
    "    plt.title('Outliers')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd6edc-533c-4e34-859b-5e7682d5cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchased_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b310b-f2a1-4520-9771-9abda92d2fb5",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "After handling the ouliers, done capping aggressively using 90%, we can observe the box plots showing no outlier points beyond the whiskers. The capping seems to have brought any extreme values within the percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d29bf4-1cca-497c-bbae-8bcc6b380d5f",
   "metadata": {},
   "source": [
    "# Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419813e-a3a2-4ccb-9058-87693894d47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the skewness by selecting the features from customer level insights\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'Tenure', 'AOV']\n",
    "\n",
    "# Checking if the selected features exist in the dataset using list comprehension\n",
    "\n",
    "customer_metrics = [col for col in customer_metrics if col in purchased_data.columns]\n",
    "\n",
    "for col in customer_metrics:\n",
    "    print(f\"{col}: {purchased_data[col].skew():.4f}\")\n",
    "    \n",
    "# Plotting histplot using kernel density estimation(KDE)\n",
    "\n",
    "for col in customer_metrics:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(purchased_data[col],kde=True,bins=50)\n",
    "    plt.title('Skewness')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5c1fa-dd1f-44a0-9c53-b73924246763",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "When we visualize the data to check for skewness, we observe that the distribution is right-skewed, also known as positively skewed, that means the tail on the right side of the distribution is longer and most of the data is present on the left side. \n",
    "Taking each features as to why they are right-skewed :\n",
    "1) **Recency :** Highly right-skewed : most customers purchased recently and few are long inactive.\n",
    "2) **Frequency :** Highly right-skewed : most buy rarely; few buy often.\n",
    "3) **Monetary :** Highly right-skewed : few customers drive most spending.\n",
    "4) **ProductVariety :**  Moderately right-skewed : most stick to few items and some explore more.\n",
    "5) **Tenure :** Fairly balanced : customers have a mix of short and long relationships.\n",
    "6) **AOV :** Moderately right-skewed : most orders are similar in value and a few are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabbd37-ad9d-4699-9b4c-bc39b5896120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling the skewness using Yeo-Johnson transformation\n",
    "\n",
    "# Checking the skewness by selecting the features from customer level insights excluding tenure.\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'AOV']\n",
    "\n",
    "# Checking if the selected features exist in the dataset using list comprehension\n",
    "\n",
    "customer_metrics = [col for col in customer_metrics if col in purchased_data.columns]\n",
    "\n",
    "# Initializing Yeo-Johnson transformer\n",
    "\n",
    "yeo_transform = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "transform = yeo_transform.fit_transform(purchased_data[customer_metrics])\n",
    "\n",
    "# Replace the columns in a copy of original DataFrame\n",
    "\n",
    "purchased_data[customer_metrics] = transform\n",
    "\n",
    "#  Skewness after Yeo-Johnson\n",
    "\n",
    "print(\"Skewness after Yeo-Johnson Transformation:\\n\")\n",
    "\n",
    "for col in customer_metrics:\n",
    "    print(f\"{col}: {purchased_data[col].skew():.4f}\")\n",
    "    \n",
    "# Plotting using hitplot\n",
    "\n",
    "for col in customer_metrics:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(purchased_data[col],kde=True,bins=30)\n",
    "    plt.title('Distribution After log transformation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d62074-f293-4568-b121-2e9b88b3d943",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "We haven't considered the Tenure in Yeo-Johnson transformation because when we checked for skewness we found out the Tenure value to be 0.4550 and it fulfills the target range for approximate symmetry (-0.5 to +0.5).\n",
    "\n",
    "The skewness value for each feature after the Yeo-Johnson transformation against the target range for approximate symmetry **-0.5 to +0.5 :** \n",
    "1) **Recency :** -0.0833 \n",
    "2) **Frequency :** 0.1608 \n",
    "3) **Monetary :** -0.0341 \n",
    "4) **ProductVariety :**  -0.0666 \n",
    "5) **AOV :** -0.0524\n",
    " \n",
    "All features now exhibit approximate symmetry, making them useful for the scaling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f0779-7a62-4b5f-91d5-86df1e5cbc42",
   "metadata": {},
   "source": [
    "# Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098a421-9086-4b0a-86af-9160e78aa502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data using standard scaler\n",
    "\n",
    "# Choosing the features to scale by from customer level insights\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'Monetary', 'ProductVariety', 'Tenure', 'AOV']\n",
    "\n",
    "#Initializing the object \n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "\n",
    "scaled_data = standard_scaler.fit_transform(purchased_data[customer_metrics])\n",
    "\n",
    "# Converting the scaled NumPy array back into a structured DataFrame\n",
    "\n",
    "convert_data = pd.DataFrame(scaled_data, columns = customer_metrics, index = purchased_data.index)\n",
    "\n",
    "# Replace the columns in a copy of original DataFrame\n",
    "\n",
    "purchased_data[customer_metrics] = convert_data\n",
    "\n",
    "# Checking Mean and Standard Deviation after scaling\n",
    "\n",
    "print(\"Mean and Standard Deviation Value : \\n\")\n",
    "\n",
    "display(purchased_data[customer_metrics].describe().loc[['mean','std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4462362-59d2-4174-8de1-74bd71c3e87f",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "We can observe that the dataset values have been successfully scaled and transformed where all the customer level insights features has a mean equal to zero and a standard deviation equal to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13536264-44ed-42f6-b926-e79c7f6a0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchased_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9648077-5697-4113-b220-b0da95119800",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d339545-a558-4b10-b6a8-9d22b888d9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting histplot for all the features\n",
    "\n",
    "for col in customer_metrics:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.histplot(purchased_data[col],kde=True,bins=30)\n",
    "    plt.title(f'Distribution After {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c877316-72ab-4461-9074-f599853a65fe",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "The distributions of all features have significantly improved, becoming more symmetrical and close to normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53de00-7b34-4b28-a683-4e605b0e344c",
   "metadata": {},
   "source": [
    "# Bivariate Analysis using Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70c510-64fc-438c-b30c-fd672d0924f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding correlation between attributes using heatmap\n",
    "\n",
    "features = purchased_data[customer_metrics]\n",
    "correlation_matrix = features.corr()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix of Attributes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3863235e-5ef8-442e-b3d6-3529d8de86a9",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "1) The heatmap shows strong positive correlations between Frequency, Monetary, Tenure, and Product Variety, indicating that customers who purchase often also tend to spend more overall, have been customers longer, and buy a wider range of products.\n",
    "2) Recency shows moderate negative correlations with these metrics, suggests new customers doesn't have high frequency or monetary value.\n",
    "3) Average Order Value is strongly correlated with Monetary and shows weaker correlations with Frequency and Tenure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d7bfe-7ab6-4b68-97fa-54c5c4a504de",
   "metadata": {},
   "source": [
    "# Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46276c-6f38-495c-a01d-0c51bdc72b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the features to scale by from customer level insights\n",
    "\n",
    "# VIF Calculation\n",
    "\n",
    "features = purchased_data[customer_metrics]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature']=features.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607c9c9-143e-4e2c-85aa-a81dccc5e00d",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "The range of high Variance Inflation Factor (VIF) values (commonly > 5 or 10) indicate strong multicollinearity. \n",
    "\n",
    "1) Monetary has value 24.29 and Frequency has value 14.57, which suggests high VIF values indicating strong multicollinearity.\n",
    "2) Monetary has value 8.75, which is high but still lower than Monetary and Frequency and suggests moderate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90192f-1fca-4de7-a3a8-c2ae2a6e2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the dataset into another variable, to use and make changes, by not affecting the scaled dataset\n",
    "\n",
    "data = purchased_data.copy()\n",
    "\n",
    "# Dropping the column Monetary \n",
    "\n",
    "data.drop(columns=['Monetary'], inplace=True)\n",
    "\n",
    "# Update customer_metrics list \n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "\n",
    "# VIF Calculation\n",
    "\n",
    "features = data[customer_metrics]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature']=features.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e2e26-1f33-44ce-bb12-23229deaea11",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "1) We dropped the Monetary column to avoid repetition, since it's closely related to Frequency and AOV. This reduces overlapping and gives accuracy.\n",
    "2) All the Variance Inflation Factor (VIF) for each numeric features are now below 5, the features are more independent and will avoid overfitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed8f27-eb4f-4de5-b05e-d753f9d945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be7dd8-59e6-43c4-9d95-c142381eefc6",
   "metadata": {},
   "source": [
    "# K-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1f736-f952-44e3-b292-579261d7518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "features = data[customer_metrics] \n",
    "\n",
    "# Using elbow method \n",
    "\n",
    "wcss = [] # Shows how spread the customers are from their group center\n",
    "\n",
    "# Take range from 1 to 10\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i, init = 'k-means++', random_state=0)\n",
    "    kmeans.fit_predict(features) # Fit the data for training the model\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS)')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba2bec-c2c3-4106-b493-d570428bc76a",
   "metadata": {},
   "source": [
    "# Observation \n",
    "\n",
    "The WCSS decreases sharply from k=1 to k=3, indicating improvement in clustering with fewer clusters. After k=3, and particularly after k=4, the rate of decrease slows down considerably, forming a visible \"elbow\" around these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77bf5e-af3b-46e8-bf3c-882a522fcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check silhouette scores\n",
    "\n",
    "# Selecting the features\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "features = data[customer_metrics] \n",
    "\n",
    "# Take range from 2 to 6\n",
    "\n",
    "for k in range(2,7):\n",
    "    kmeans = KMeans(n_clusters=k, init = 'k-means++', random_state=0)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    score = silhouette_score(features,labels)\n",
    "    print(f'k={k}, Silhouette Score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb8dba-511b-4778-86de-6d51b0924dd2",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "1) Silhouette Score tells you how well-separated the clusters are. Silhouette Score always ranges from -1 to +1. A score of 0.37 is positive and closer to 0 than to 1, indicates theres overlapping.\n",
    "2) The Silhouette Score is highest for k=2 (0.3699) compared to others, so we choose this value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb0f01-ec1e-42a3-8d1c-397857d9ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "features = data[customer_metrics] \n",
    "\n",
    "# List of cluster(k) values to iterate over\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=2, init = 'k-means++', random_state=0)\n",
    "labels_final = kmeans_final.fit_predict(features)\n",
    "\n",
    "# Add cluster labels to original data\n",
    "    \n",
    "data['Cluster_KMeans'] = labels_final\n",
    "\n",
    "# Displaying the calculated size of the clusters\n",
    "\n",
    "print(f\"Cluster Sizes when k={2}:\\n\")\n",
    "print(data['Cluster_KMeans'].value_counts(),\"\\n\")\n",
    "\n",
    "# Displaying the calculated cluster-wise means of all the features from the dataset\n",
    "\n",
    "cluster_feature = data.groupby('Cluster_KMeans')[customer_metrics].mean()\n",
    "print(f\"Feature Averages when k={2}:\\n\", cluster_feature,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be58bc9-2b25-4ff0-876d-b2d3f32e5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "features_transform = pca.fit_transform(features)\n",
    "\n",
    "# Plot scatterplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter_plot = plt.scatter(features_transform[:, 0], features_transform[:, 1], c=labels_final, cmap='viridis')\n",
    "\n",
    "#Add legends to distinguish\n",
    "plt.scatter([],[], c=\"yellow\",label=\"Feature-1\")\n",
    "plt.scatter([],[], c=\"green\",label=\"Feature-2\")\n",
    "plt.colorbar(scatter_plot)\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6801a5-9a71-4bf8-9560-f3f3ed27fe8f",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "**For k=2**\n",
    "\n",
    "1) We can't directly plot 5 features on a standard 2D scatter plot, so we use PCA, which reduces 5 features into 2 axis.\n",
    "2) Cluster 0 (2304 Customers): This cluster represents customers who have low frequency of purchases and low product variety interest, but they have been with you for a moderate amount of time (average tenure) and show some engagement.\n",
    "3) Cluster 1 (2035 Customers): This cluster represents your most engaged customers with high frequency, long tenure, and high interest in product variety. These are the \"loyal\" customers who likely spend more per order (higher AOV) and have been with you for a while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc06520-b610-49d5-af3f-c293cacf4845",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "In the next modelling technique will be DBSCAN, because the K-Means results specifically the low Silhouette scores across different 'k' values suggest K-Means is not able to find very distinct and separated clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dde01-77b9-4b7b-9c43-4eea8dc70071",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970a6ac-4f1c-4f3d-a49b-3e8777adc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for DBSCAN \n",
    "\n",
    "# Selecting the features\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "features = data[customer_metrics]  \n",
    "\n",
    "# Implement k nearest neighbors\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=10)\n",
    "neighbors_fit=neighbors.fit(features)\n",
    "\n",
    "# Extract Distances and Indices of data points\n",
    "\n",
    "distances,indices = neighbors_fit.kneighbors(features)\n",
    "\n",
    "# Sort the distance to kth nearest neighbor in ascending order\n",
    "\n",
    "distances = np.sort(distances[:,9]) # Taking 9 because n_neighbors is 10\n",
    "\n",
    "plt.plot(distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df6626-fc5f-48a1-849e-e44d05e34a70",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "1) Taken higher min_samples(k) = 10, as it helps DBSCAN algorithm to ignore random noise and focus on more dense areas.\n",
    "2) This plot shows the sorted distances to the 10th nearest neighbor. Looking at the elbow curve where it has the bend at 0.6 onwards. To evaluate this further lets consider eps=0.6. Also lets find the number of cluters excluding noise points and the number of noise points labeled as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb91312-42d8-40b7-9215-19b870325d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the features\n",
    "\n",
    "customer_metrics = ['Recency', 'Frequency', 'ProductVariety', 'Tenure', 'AOV']\n",
    "features = data[customer_metrics] \n",
    "\n",
    "# Using DBSCAN clustering\n",
    "\n",
    "print(f\"DBSCAN results when eps : 0.6 \\n\")\n",
    "dbscan = DBSCAN(eps=0.6,min_samples=10) \n",
    "labels_dbscan = dbscan.fit_predict(features) \n",
    "\n",
    "# Add the cluster labels to the original data\n",
    "    \n",
    "data['Cluster_DBSCAN'] = labels_dbscan \n",
    "\n",
    "# Calculate the number of clusters excluding noise points also labelling the noise points as -1\n",
    "\n",
    "num_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "num_noise = list(labels_dbscan).count(-1)\n",
    "    \n",
    "# Displaying the number of clusters excluding noise and noise points\n",
    "    \n",
    "print(f\"Number of clusters excluding noise: {num_clusters}\")\n",
    "print(f\"Number of noise points (labeled as -1): {num_noise}\\n\")\n",
    "\n",
    "# Displaying the calculated size of the clusters\n",
    "\n",
    "print(\"Cluster Sizes when DBSCAN:\")\n",
    "print(data['Cluster_DBSCAN'].value_counts(), \"\\n\")\n",
    "\n",
    "# Displaying the calculated cluster-wise means of all features\n",
    "\n",
    "cluster_feature = data.groupby('Cluster_DBSCAN')[customer_metrics].mean()\n",
    "print(\"Feature Averages when DBSCAN:\\n\", cluster_feature,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b765510-a1e1-4e52-81a6-c59e91917016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "features_transform = pca.fit_transform(features)\n",
    "\n",
    "# Plot scatterplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter_plot = plt.scatter(features_transform[:, 0], features_transform[:, 1], c=labels_dbscan, cmap='viridis')\n",
    "\n",
    "#Add legends to distinguish\n",
    "plt.scatter([],[], c=\"yellow\",label=\"Feature-1\")\n",
    "plt.scatter([],[], c=\"green\",label=\"Feature-2\")\n",
    "plt.scatter([],[], c=\"purple\",label=\"Noise\")\n",
    "plt.colorbar(scatter_plot)\n",
    "plt.title('DBSCAN Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b330a-adc7-45da-a770-50b12bbcfab7",
   "metadata": {},
   "source": [
    "### Observation \n",
    "\n",
    "**For eps=0.6**\n",
    "\n",
    "1) We can't directly plot 5 features on a standard 2D scatter plot, so we use PCA, which reduces 5 features into 2 axis.\n",
    "2)  As eps increases, the number of noise points decreases, and the size of main clusters increases.\n",
    "3) Clusters:\n",
    "    - Cluster 0: Remains stable, suggests less engaged customers, with less frequency value.\n",
    "    - Cluster 1: Grows as eps increases, suggests more engaged and loyal customers, buying recently and frequently.\n",
    "    - Cluster -1: Outliers that don't fit into the main customer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896bb71-0c04-4dfd-a487-15bd5afebe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first five rows of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8339c6-a748-4a4c-9395-70f9bcb0689d",
   "metadata": {},
   "source": [
    "# Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddac2c-8f9b-4a6f-896a-a34e4c676c5e",
   "metadata": {},
   "source": [
    "## KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14769e-b228-40ca-9d2e-9b776961f5b2",
   "metadata": {},
   "source": [
    "**For k=2 Clusters**:\n",
    "\n",
    "    - Insights Cluster 0: Inactive customers with lower engagement.\n",
    "      Improvise: Target with promotions to increase activity.\n",
    "\n",
    "    - InsightsCluster 1: Loyal, high-value customers.\n",
    "      Improvise: Retain with loyalty programs and exclusive offers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158b9c-89b6-4144-8e95-9b29e2a09cbd",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3f585-bae0-42d0-b948-cfaa08c520f4",
   "metadata": {},
   "source": [
    "**For DBSCAN (eps = 0.6):**\n",
    "\n",
    "    - Insights Cluster 0: Inactive customers with lower engagement.  \n",
    "      Improvise: Target with promotions to increase activity.\n",
    "\n",
    "    - Insights Cluster 1: Active customers with higher engagement and variety.  \n",
    "      Improvise: Retain with loyalty programs and personalized offers.\n",
    "\n",
    "    - Insights Noise points: 653 data points that do not belong to any cluster.  \n",
    "      Improvise: Investigate potential outliers or irrelevant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5428ce3e-0781-402c-90eb-aa219033efc0",
   "metadata": {},
   "source": [
    "**From both K-Means and DBSCAN we derive the Business Strategy**\n",
    "\n",
    "1) Re-engage Inactive Customers (Cluster 0): These customers aren’t as active, but they still have potential. Bring them back with special offers or targeted campaigns. A good re-engagement strategy can help you win them over and boost retention.\n",
    "2) Retain Active Customers (Cluster 1): These are your loyal, high-value customers. Keep them happy with loyalty programs, personalized offers, and exclusive deals to make sure they stay engaged and keep coming back.\n",
    "3) Look Into Noise Points for DBSCAN: Noise points are outliers that don’t fit into clusters, you can decide whether to target them or clean up the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae4a18c-6911-45ed-935c-9c2e6bfc6a11",
   "metadata": {},
   "source": [
    "## Steps for saving the data for creating api's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc26e36-8909-4646-82af-189b7112992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "\n",
    "os.makedirs(\"models\",exist_ok=True)\n",
    "os.makedirs(\"data\",exist_ok=True)\n",
    "\n",
    "# Saving the model tained K-Means using joblib\n",
    "\n",
    "joblib.dump(kmeans_final,\"models/kmeans_model.pkl\")\n",
    "\n",
    "# Copying the dataset into another variable, to use and make changes for creating api, by not affecting the modeled dataset\n",
    "api_data = data.copy()\n",
    "\n",
    "# Setting Customer ID as index and then writing to csv\n",
    "\n",
    "api_data.set_index(\"CustomerID\", inplace=True) \n",
    "api_data.to_csv(\"data/apidata.csv\")\n",
    "\n",
    "#Display the dataset \n",
    "\n",
    "print(api_data.head())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d20746-49cf-4d72-ab4a-d02e49b15910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
